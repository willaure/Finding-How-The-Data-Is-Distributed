# -*- coding: utf-8 -*-
"""Untitled36.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrXiP99AmjjVwIWfe27c4-nUqvMxYY61
"""

# Install the required libraries

# Standard imports for Colab (no installation needed)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Import Libraries and Load Data¶
# Import the pandas, matplotlib.pyplot, and seaborn libraries.
# You will begin with loading the dataset. You can use the pyfetch method if working on JupyterLite. Otherwise, you can use pandas' read_csv() function directly on their local machines or cloud environments.

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Stack Overflow survey dataset
data_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv'
df = pd.read_csv(data_url)

# Display the first few rows of the dataset
df.head()

# Step 2: Examine the Structure of the Data¶
# Display the column names, data types, and summary information to understand the data structure.
# Objective: Gain insights into the dataset's shape and available variables.

# Step 2: Examine the Data Structure

# 1. Display basic dataset info
print("="*50)
print("DATASET SHAPE")
print("="*50)
print(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}\n")

# 2. Show column names and data types
print("="*50)
print("COLUMN INFORMATION")
print("="*50)
print(df.dtypes.to_string())  # to_string() prevents truncation

# 3. Display summary statistics
print("\n" + "="*50)
print("NUMERICAL COLUMNS SUMMARY")
print("="*50)
print(df.describe(include='number').to_string())

print("\n" + "="*50)
print("CATEGORICAL COLUMNS SUMMARY")
print("="*50)
print(df.describe(include='object').to_string())

# 4. Show memory usage
print("\n" + "="*50)
print("MEMORY USAGE")
print("="*50)
print(df.memory_usage(deep=True).to_string())

# 5. Check for missing values
print("\n" + "="*50)
print("MISSING VALUES")
print("="*50)
print(df.isna().sum().sort_values(ascending=False).to_string())

# 6. Display first and last few rows
print("\n" + "="*50)
print("DATA PREVIEW (First 3 and Last 3 Rows)")
print("="*50)
display(df.head(3))
display(df.tail(3))

# Step 3: Handle Missing Data¶
# Identify missing values in the dataset.
# Impute or remove missing values as necessary to ensure data completeness.

# Step 3: Handle Missing Data

# 1. Initial missing value analysis
print("="*50)
print("BEFORE MISSING DATA HANDLING")
print("="*50)
missing_before = df.isna().sum().sort_values(ascending=False)
missing_percent = (df.isna().mean() * 100).sort_values(ascending=False)
missing_report = pd.concat([missing_before, missing_percent], axis=1, keys=['Count', 'Percent'])
display(missing_report[missing_report['Count'] > 0].style.background_gradient(cmap='Reds'))

# 2. Define handling strategy for each column type
def handle_missing(df):
    df_clean = df.copy()

    # A. Drop columns with too much missing data (threshold: 70%)
    cols_to_drop = missing_percent[missing_percent > 70].index
    df_clean = df_clean.drop(columns=cols_to_drop)
    print(f"Dropped columns with >70% missing: {list(cols_to_drop)}")

    # B. Handle numerical columns
    num_cols = df_clean.select_dtypes(include=['number']).columns
    for col in num_cols:
        if df_clean[col].isna().any():
            # Use median for skewed distributions
            if abs(df_clean[col].skew()) > 1:
                fill_value = df_clean[col].median()
                method = 'median'
            else:
                fill_value = df_clean[col].mean()
                method = 'mean'
            df_clean[col] = df_clean[col].fillna(fill_value)
            print(f"Filled {col} missing values with {method}: {fill_value:.2f}")

    # C. Handle categorical columns
    cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns
    for col in cat_cols:
        if df_clean[col].isna().any():
            # For low cardinality, use mode
            if df_clean[col].nunique() < 10:
                fill_value = df_clean[col].mode()[0]
                df_clean[col] = df_clean[col].fillna(fill_value)
                print(f"Filled {col} missing values with mode: {fill_value}")
            else:
                # For high cardinality, create 'Missing' category
                df_clean[col] = df_clean[col].fillna('Missing')
                print(f"Filled {col} missing values with 'Missing' category")

    # D. Handle datetime columns
    if 'datetime' in df_clean.dtypes.values:
        date_cols = df_clean.select_dtypes(include=['datetime']).columns
        for col in date_cols:
            if df_clean[col].isna().any():
                df_clean[col] = df_clean[col].fillna(df_clean[col].median())
                print(f"Filled {col} missing dates with median date")

    return df_clean

# 3. Apply missing data handling
df_clean = handle_missing(df)

# 4. Verify handling
print("\n" + "="*50)
print("AFTER MISSING DATA HANDLING")
print("="*50)
missing_after = df_clean.isna().sum().sort_values(ascending=False)
display(missing_after[missing_after > 0].to_frame('Remaining Missing Values'))

# 5. Optional: Create missing value flags for important columns
important_cols = ['Salary', 'JobSatisfaction', 'Education']  # customize with your important columns
for col in important_cols:
    if col in df.columns:
        df_clean[f'{col}_was_missing'] = df[col].isna()

print("\nMissing data handling complete!")

# Step 4: Analyze Key Columns¶
# Examine key columns such as Employment, JobSat (Job Satisfaction), and YearsCodePro (Professional Coding Experience).
# Instruction: Calculate the value counts for each column to understand the distribution of responses.

# Step 4: Analyze Key Columns
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_column(column_name, df, top_n=10):
    """Enhanced column analysis with visualization"""
    print(f"\n{'='*50}\nAnalysis of: {column_name}\n{'='*50}")

    # 1. Basic value counts - FIXED SYNTAX
    counts = df[column_name].value_counts(dropna=False).head(top_n)
    percentages = df[column_name].value_counts(normalize=True, dropna=False).head(top_n) * 100  # Removed extra parenthesis

    stats = pd.concat([counts, percentages], axis=1,
                     keys=['Count', 'Percentage'])
    display(stats.style.format({'Percentage': '{:.1f}%'}))

    # 2. Enhanced visualization
    plt.figure(figsize=(12, 6))

    # For categorical data
    if df[column_name].dtype == 'object':
        # Get top N categories
        top_categories = df[column_name].value_counts().head(top_n).index
        filtered_data = df[df[column_name].isin(top_categories)]

        # Create ordered countplot
        ax = sns.countplot(y=column_name, data=filtered_data,
                          order=top_categories,
                          palette='viridis')

        # Add annotations
        total = len(filtered_data)
        for p in ax.patches:
            width = p.get_width()
            ax.text(width + 0.02*total, p.get_y() + p.get_height()/2,
                   f'{width:,}\n({width/total:.1%})',
                   va='center')

        plt.title(f'Distribution of {column_name} (Top {top_n})', pad=20)
        plt.xlabel('Count')
        plt.ylabel(column_name)

    # For numerical data
    else:
        # Histogram + KDE plot
        sns.histplot(df[column_name], kde=True, color='skyblue')
        plt.title(f'Distribution of {column_name}', pad=20)
        plt.xlabel(column_name)
        plt.ylabel('Frequency')

        # Add statistical annotations
        stats_text = f"""
        Mean: {df[column_name].mean():.2f}
        Median: {df[column_name].median():.2f}
        Std Dev: {df[column_name].std():.2f}
        """
        plt.gcf().text(0.7, 0.7, stats_text, bbox=dict(facecolor='white', alpha=0.8))

    plt.tight_layout()
    plt.show()

# Analyze each key column
print("Starting analysis of key columns...")
analyze_column('Employment', df)
analyze_column('JobSat', df)
analyze_column('YearsCodePro', df)

# Additional analysis for YearsCodePro if it contains categorical responses
if df['YearsCodePro'].dtype == 'object':
    print("\nFound categorical YearsCodePro values - converting to numerical:")
    print("Unique values:", df['YearsCodePro'].unique())

    # Convert to numerical (handling categorical ranges)
    df['YearsCodePro_num'] = df['YearsCodePro'].replace({
        'Less than 1 year': 0.5,
        'More than 50 years': 50
    }).astype(float)

    # Re-analyze as numerical
    analyze_column('YearsCodePro_num', df)
else:
    print("\nYearsCodePro is already numerical - no conversion needed")

print("\nAnalysis completed successfully!")

# Step 5: Visualize Job Satisfaction (Focus on JobSat)¶
# Create a pie chart or KDE plot to visualize the distribution of JobSat.
# Provide an interpretation of the plot, highlighting key trends in job satisfaction.

# Step 5: Visualize Job Satisfaction Distribution - Fixed Version
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Clean and prepare the data
df_jobsat = df.dropna(subset=['JobSat']).copy()

# Get unique categories in your actual data
unique_jobsat = df_jobsat['JobSat'].unique()
print("Unique JobSat values in your data:", unique_jobsat)

# 2. Create pie chart that adapts to your data
plt.figure(figsize=(10, 8))

# Calculate counts
job_sat_counts = df_jobsat['JobSat'].value_counts()

# Create explode array dynamically
explode = [0.05] * len(job_sat_counts)  # Adjust based on your category count

# Create color palette
colors = plt.cm.Paired.colors[:len(job_sat_counts)]

# Create pie chart
patches, texts, autotexts = plt.pie(
    job_sat_counts,
    labels=job_sat_counts.index,
    colors=colors,
    autopct='%1.1f%%',
    startangle=90,
    explode=explode,
    shadow=True,
    textprops={'fontsize': 10}
)

# Improve readability
plt.title('Job Satisfaction Distribution', pad=20, fontsize=14)
plt.tight_layout()
plt.show()

# 3. Create bar chart alternative (more precise)
plt.figure(figsize=(12, 6))
ax = sns.countplot(y='JobSat', data=df_jobsat,
                  order=job_sat_counts.index,
                  palette='viridis')

# Add percentages
total = len(df_jobsat)
for p in ax.patches:
    width = p.get_width()
    ax.text(width + 0.02*total, p.get_y() + p.get_height()/2,
           f'{width} ({width/total:.1%})',
           va='center')

plt.title('Job Satisfaction Distribution (Exact Counts)', pad=20, fontsize=14)
plt.xlabel('Count')
plt.ylabel('Satisfaction Level')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

# 4. Interpretation
print("\nKey Insights:")
print("-"*50)
print(f"1. Most common response: {job_sat_counts.index[0]} ({job_sat_counts.iloc[0]/total:.1%})")
print(f"2. Least common response: {job_sat_counts.index[-1]} ({job_sat_counts.iloc[-1]/total:.1%})")
print(f"3. Satisfaction ratio: {(job_sat_counts.get('Slightly satisfied', 0) + job_sat_counts.get('Very satisfied', 0)) / total:.1%}")
print(f"4. Dissatisfaction ratio: {(job_sat_counts.get('Slightly dissatisfied', 0) + job_sat_counts.get('Very dissatisfied', 0)) / total:.1%}")

# Step 6: Programming Languages Analysis
# Compare the frequency of programming languages in LanguageHaveWorkedWith and LanguageWantToWorkWith.
# Visualize the overlap or differences using a Venn diagram or a grouped bar chart.

# Step 6: Compare Programming Language Preferences
import ast
from matplotlib_venn import venn2
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Data Preparation
def parse_languages(lang_str):
    """Convert string representation of list to actual list"""
    try:
        return ast.literal_eval(lang_str) if pd.notna(lang_str) else []
    except:
        return str(lang_str).split(';') if pd.notna(lang_str) else []

# Parse language columns
df['LanguagesWorked'] = df['LanguageHaveWorkedWith'].apply(parse_languages)
df['LanguagesDesired'] = df['LanguageWantToWorkWith'].apply(parse_languages)

# 2. Get top languages (appearing in at least 5% of responses)
def get_top_languages(series, threshold=0.05):
    all_langs = [lang for sublist in series for lang in sublist]
    lang_counts = Counter(all_langs)
    total = len(series)
    return {lang: count for lang, count in lang_counts.items() if count/total >= threshold}

top_worked = get_top_languages(df['LanguagesWorked'])
top_desired = get_top_languages(df['LanguagesDesired'])

# 3. Venn Diagram
plt.figure(figsize=(10, 8))
venn2([set(top_worked.keys()), set(top_desired.keys())],
     set_labels=('Currently Using', 'Want to Use'),
     set_colors=('#1f77b4', '#ff7f0e'),
     alpha=0.7)
plt.title('Overlap Between Current and Desired Languages\n(Top Languages Only)', pad=20)
plt.show()

# 4. Grouped Bar Chart
# Prepare data for visualization
worked_df = pd.DataFrame.from_dict(top_worked, orient='index', columns=['WorkedWith'])
desired_df = pd.DataFrame.from_dict(top_desired, orient='index', columns=['WantToWorkWith'])
lang_df = worked_df.join(desired_df, how='outer').fillna(0)
lang_df = lang_df.sort_values('WorkedWith', ascending=False)

# Normalize to percentages
total_respondents = len(df)
lang_df = lang_df / total_respondents * 100

# Create plot
plt.figure(figsize=(14, 8))
ax = lang_df.plot(kind='bar', color=['#1f77b4', '#ff7f0e'], width=0.8)
plt.title('Programming Language Popularity: Current vs Desired', pad=20, fontsize=14)
plt.xlabel('Programming Language', fontsize=12)
plt.ylabel('Percentage of Respondents', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.3)
plt.legend(['Currently Using', 'Want to Use'], title='Language Status')

# Add value labels
for p in ax.patches:
    if p.get_height() > 1:  # Only label bars >1% to avoid clutter
        ax.annotate(f'{p.get_height():.1f}%',
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, 5),
                   textcoords='offset points',
                   fontsize=9)

plt.tight_layout()
plt.show()

# 5. Top Gaining/Losing Languages
lang_df['Difference'] = lang_df['WantToWorkWith'] - lang_df['WorkedWith']
trend_df = lang_df.sort_values('Difference', ascending=False)

print("\nTop 5 Languages Gaining Popularity:")
display(trend_df.head(5).style.format('{:.1f}%'))

print("\nTop 5 Languages Losing Popularity:")
display(trend_df.tail(5).style.format('{:.1f}%'))

# Step 7: Analyze Remote Work Trends¶
# Visualize the distribution of RemoteWork by region using a grouped bar chart or heatmap.

# Step 7: Analyze Remote Work Trends by Region - Robust Version
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Data Preparation
df_remote = df.dropna(subset=['RemoteWork', 'Country']).copy()

# Standardize remote work categories based on what exists in your data
existing_categories = df_remote['RemoteWork'].unique()
print("Existing RemoteWork categories:", existing_categories)

remote_mapping = {
    'Fully remote': 'Remote',
    'Hybrid (some remote, some in-person)': 'Hybrid',
    'Full in-person': 'In-office'
}

# Only apply mappings that exist in your data
df_remote['RemoteWork'] = df_remote['RemoteWork'].replace(
    {k: v for k, v in remote_mapping.items() if k in existing_categories}
)

# Get current categories after mapping
current_categories = df_remote['RemoteWork'].unique()
print("Mapped RemoteWork categories:", current_categories)

# Get top N regions by response count
top_countries = df_remote['Country'].value_counts().head(15).index
df_top = df_remote[df_remote['Country'].isin(top_countries)]

# 2. Grouped Bar Chart
plt.figure(figsize=(14, 8))
sns.countplot(
    y='Country',
    hue='RemoteWork',
    data=df_top,
    order=top_countries,
    hue_order=sorted(current_categories),  # Use actual categories present
    palette='coolwarm'
)

plt.title('Remote Work Preferences by Country (Top 15)', pad=20, fontsize=14)
plt.xlabel('Number of Respondents', fontsize=12)
plt.ylabel('Country', fontsize=12)
plt.legend(title='Work Preference', bbox_to_anchor=(1.05, 1), loc='upper left')

# Add percentage annotations
total_by_country = df_top['Country'].value_counts()
for container in plt.gca().containers:
    labels = []
    for bar in container:
        country = bar.get_y() + bar.get_height()/2
        country = top_countries[int(country)]
        percentage = 100 * bar.get_width() / total_by_country[country]
        labels.append(f'{bar.get_width()}\n({percentage:.1f}%)')
    plt.bar_label(container, labels=labels, label_type='center', fontsize=9)

plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

# 3. Heatmap (Normalized by Country) - Only if we have multiple categories
if len(current_categories) > 1:
    plt.figure(figsize=(12, 8))

    # Create normalized cross-tab with existing categories
    remote_pivot = pd.crosstab(
        index=df_top['Country'],
        columns=df_top['RemoteWork'],
        normalize='index'
    ).loc[top_countries, sorted(current_categories)]  # Sort categories alphabetically

    sns.heatmap(
        remote_pivot,
        annot=True,
        fmt='.0%',
        cmap='YlGnBu',
        linewidths=.5,
        cbar_kws={'label': 'Percentage'}
    )

    plt.title('Remote Work Distribution by Country (%)', pad=20, fontsize=14)
    plt.xlabel('Work Preference', fontsize=12)
    plt.ylabel('Country', fontsize=12)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    # Statistical Summary
    if 'Remote' in remote_pivot.columns:
        print("\nRemote Work Adoption Leaders:")
        print(remote_pivot['Remote'].sort_values(ascending=False).head(5)
              .to_string(float_format='{:.1%}'.format))
else:
    print("\nOnly one work preference category available - skipping heatmap")

# 4. Regional Analysis (if Continent column exists)
if 'Continent' in df.columns:
    plt.figure(figsize=(12, 6))
    sns.countplot(
        y='Continent',
        hue='RemoteWork',
        data=df_remote,
        hue_order=sorted(current_categories),
        palette='coolwarm'
    )
    plt.title('Remote Work Preferences by Continent', fontsize=14)
    plt.xlabel('Number of Respondents')
    plt.ylabel('Continent')
    plt.tight_layout()
    plt.show()

# Step 8: Correlation between Job Satisfaction and Experience
# Analyze the correlation between overall job satisfaction (JobSat) and YearsCodePro.
# Calculate the Pearson or Spearman correlation coefficient.

# Step 8: Correlation Between Job Satisfaction and Experience
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# 1. Data Preparation
df_corr = df.dropna(subset=['JobSat', 'YearsCodePro']).copy()

# Convert YearsCodePro to numeric (handling categorical responses)
years_code_map = {
    'Less than 1 year': 0.5,
    'More than 50 years': 50
}

# Apply only to existing categories
existing_years = {k: v for k, v in years_code_map.items()
                 if k in df_corr['YearsCodePro'].unique()}
df_corr['YearsCodePro'] = df_corr['YearsCodePro'].replace(existing_years).astype(float)

# Convert JobSat to ordinal numeric scale
job_sat_order = {
    'Very dissatisfied': 1,
    'Slightly dissatisfied': 2,
    'Neither satisfied nor dissatisfied': 3,
    'Slightly satisfied': 4,
    'Very satisfied': 5
}

# Apply only to existing categories
existing_sat = {k: v for k, v in job_sat_order.items()
               if k in df_corr['JobSat'].unique()}
df_corr['JobSatNumeric'] = df_corr['JobSat'].replace(existing_sat)

# 2. Calculate Correlation Coefficients
pearson_corr, pearson_p = stats.pearsonr(df_corr['YearsCodePro'], df_corr['JobSatNumeric'])
spearman_corr, spearman_p = stats.spearmanr(df_corr['YearsCodePro'], df_corr['JobSatNumeric'])

print("Correlation Analysis Results:")
print("-"*40)
print(f"Pearson r: {pearson_corr:.3f} (p-value: {pearson_p:.4f})")
print(f"Spearman ρ: {spearman_corr:.3f} (p-value: {spearman_p:.4f})")

# 3. Scatter Plot with Regression Line
plt.figure(figsize=(12, 7))
sns.regplot(
    x='YearsCodePro',
    y='JobSatNumeric',
    data=df_corr,
    scatter_kws={'alpha': 0.4, 'color': 'steelblue'},
    line_kws={'color': 'red', 'linewidth': 2},
    ci=95
)

# Add correlation info to plot
plt.text(
    0.05, 0.95,
    f"Pearson r = {pearson_corr:.2f} (p = {pearson_p:.3f})\n"
    f"Spearman ρ = {spearman_corr:.2f} (p = {spearman_p:.3f})",
    transform=plt.gca().transAxes,
    bbox=dict(facecolor='white', alpha=0.8)
)

# Convert y-axis back to original labels
sat_labels = sorted(existing_sat.items(), key=lambda x: x[1])
plt.yticks(
    ticks=[x[1] for x in sat_labels],
    labels=[x[0] for x in sat_labels]
)

plt.title('Job Satisfaction vs. Professional Coding Experience', fontsize=14)
plt.xlabel('Years of Professional Coding', fontsize=12)
plt.ylabel('Job Satisfaction', fontsize=12)
plt.grid(alpha=0.2)
plt.tight_layout()
plt.show()

# 4. Binned Analysis (for non-linear relationships)
df_corr['ExperienceBins'] = pd.cut(
    df_corr['YearsCodePro'],
    bins=[0, 2, 5, 10, 20, np.inf],
    labels=['0-2', '2-5', '5-10', '10-20', '20+']
)

plt.figure(figsize=(12, 6))
sns.boxplot(
    x='ExperienceBins',
    y='JobSatNumeric',
    data=df_corr,
    palette='viridis'
)

# Add median labels
medians = df_corr.groupby('ExperienceBins')['JobSatNumeric'].median()
for i, (bin_name, median_val) in enumerate(medians.items()):
    plt.text(i, median_val, f'{median_val:.1f}',
             ha='center', va='bottom', color='white', weight='bold')

plt.title('Job Satisfaction Distribution by Experience Level', fontsize=14)
plt.xlabel('Years of Professional Coding', fontsize=12)
plt.ylabel('Job Satisfaction Score', fontsize=12)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# 5. Interpretation
print("\nKey Insights:")
print("-"*40)
print(f"1. Correlation Strength: {'Weak' if abs(pearson_corr) < 0.3 else 'Moderate' if abs(pearson_corr) < 0.5 else 'Strong'}")
print(f"2. Direction: {'Positive' if pearson_corr > 0 else 'Negative'} relationship")
print(f"3. Significance: {'Statistically significant' if pearson_p < 0.05 else 'Not statistically significant'}")
print("4. See boxplot for non-linear patterns across experience levels")

# Step 9: Cross-tabulation Analysis (Employment vs. Education Level)
# Analyze the relationship between employment status (Employment) and education level (EdLevel).
# Instruction: Create a cross-tabulation using pd.crosstab() and visualize it with a stacked bar plot if possible.

# Step 9: Employment vs. Education Level Analysis
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Data Preparation
df_edu_emp = df.dropna(subset=['Employment', 'EdLevel']).copy()

# Standardize education levels (customize based on your actual data)
education_map = {
    'No formal education': 'No formal education',
    'Primary/elementary school': 'Primary',
    'Secondary school': 'Secondary',
    'Some college/university': 'Some college',
    'Associate degree': 'Associate',
    'Bachelor’s degree': "Bachelor's",
    'Master’s degree': "Master's",
    'Professional degree': 'Professional',
    'Doctoral degree': 'Doctoral'
}

# Apply only to existing categories
existing_edu = {k: v for k, v in education_map.items()
               if k in df_edu_emp['EdLevel'].unique()}
df_edu_emp['EdLevel'] = df_edu_emp['EdLevel'].replace(existing_edu)

# Define education level order
edu_order = [
    'No formal education',
    'Primary',
    'Secondary',
    'Some college',
    'Associate',
    "Bachelor's",
    "Master's",
    'Professional',
    'Doctoral'
]

# Filter to only existing education levels
edu_order = [x for x in edu_order if x in df_edu_emp['EdLevel'].unique()]

# 2. Cross-tabulation Analysis
print("Cross-tabulation of Employment vs. Education Level:")
cross_tab = pd.crosstab(
    index=df_edu_emp['Employment'],
    columns=df_edu_emp['EdLevel'],
    margins=True,
    margins_name="Total"
)

# Display with percentages
cross_tab_pct = pd.crosstab(
    index=df_edu_emp['Employment'],
    columns=df_edu_emp['EdLevel'],
    normalize='index'
).round(3) * 100

display(cross_tab.style.background_gradient(cmap='Blues', axis=1))
print("\nPercentage Distribution:")
display(cross_tab_pct.style.format('{:.1f}%').background_gradient(cmap='Greens', axis=1))

# 3. Stacked Bar Plot Visualization
plt.figure(figsize=(14, 8))

# Get top employment statuses
top_employment = df_edu_emp['Employment'].value_counts().head(8).index
df_top = df_edu_emp[df_edu_emp['Employment'].isin(top_employment)]

# Create plot
cross_tab_top = pd.crosstab(
    index=df_top['Employment'],
    columns=df_top['EdLevel']
).loc[top_employment, edu_order]

ax = cross_tab_top.plot(
    kind='barh',
    stacked=True,
    figsize=(14, 8),
    cmap='viridis'
)

# Add percentage annotations
for container in ax.containers:
    # Calculate percentages
    labels = []
    totals = cross_tab_top.sum(axis=1)
    for i, bar in enumerate(container):
        emp_type = bar.get_y() + bar.get_height()/2
        emp_type = top_employment[int(emp_type)]
        percentage = 100 * bar.get_width() / totals[emp_type]
        labels.append(f'{percentage:.1f}%' if bar.get_width() > 0 else '')
    ax.bar_label(container, labels=labels, label_type='center', fontsize=9)

plt.title('Education Level Distribution by Employment Status', fontsize=16)
plt.xlabel('Number of Respondents', fontsize=12)
plt.ylabel('Employment Status', fontsize=12)
plt.legend(title='Education Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

# 4. Statistical Summary
print("\nMost Common Education Levels by Employment Status:")
display(
    df_edu_emp.groupby('Employment')['EdLevel']
    .agg(lambda x: x.mode()[0])
    .value_counts()
    .to_frame('Number of Employment Types')
    .rename_axis('Most Common Education Level')
)

# 5. Heatmap Alternative
plt.figure(figsize=(12, 8))
sns.heatmap(
    cross_tab_top.div(cross_tab_top.sum(axis=1), axis=0),
    annot=True,
    fmt='.1%',
    cmap='YlGnBu',
    linewidths=.5
)
plt.title('Education Level Proportions by Employment Status', fontsize=14)
plt.xlabel('Education Level')
plt.ylabel('Employment Status')
plt.tight_layout()
plt.show()

# Step 10: Export Cleaned Data
# Save the cleaned dataset to a new CSV file for further use or sharing.

# Let's assume your cleaned DataFrame is called df_cleaned
# (Replace df_cleaned with your actual DataFrame name if different)

# Export to CSV
df.to_csv('cleaned_survey_data.csv', index=False)

print("✅ Cleaned dataset exported successfully to 'cleaned_survey_data.csv'")